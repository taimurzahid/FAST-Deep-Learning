{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "First_Part.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ceqx9QksPgxg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Mounting Google drive"
      ]
    },
    {
      "metadata": {
        "id": "PqNB5Rzl5MZ3",
        "colab_type": "code",
        "outputId": "c4a1a6e4-236f-43a1-ae51-d77442ca7240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cXCXFNV26-ax",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Import keras and other python libraries"
      ]
    },
    {
      "metadata": {
        "id": "DRZdRmi5WJZo",
        "colab_type": "code",
        "outputId": "6b41e7c8-3fa2-44a0-e69a-6a9939fbeb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from keras.callbacks import TensorBoard\n",
        "import os\n",
        "from keras.optimizers import SGD\n",
        "from os import listdir\n",
        "from os.path import join, basename\n",
        "from keras.preprocessing.image import ImageDataGenerator,load_img,array_to_img, img_to_array\n",
        "from keras.regularizers import l2\n",
        "from keras import regularizers\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EDSG3DCzrb8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#address to the drive directory"
      ]
    },
    {
      "metadata": {
        "id": "KA7CUT7bW9d-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dirc='/gdrive/My Drive/Final Project/DermCNN data/'\n",
        "labels=[]\n",
        "data=[]\n",
        "im_width=150\n",
        "im_height=150"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NAZ7IJAX7R0l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Fetching in data and resizing each image on the size 150*150"
      ]
    },
    {
      "metadata": {
        "id": "1j8YelwpW9td",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(7):\n",
        "    path=os.getcwd()+dirc+str(i+1)+'/'\n",
        "    print(path)\n",
        "    progress = 0\n",
        "    image_files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path,f))]\n",
        "    for file_name in image_files:\n",
        "        image_file = str(path + file_name)\n",
        "        img = cv2.imread(image_file)\n",
        "        new_img = cv2.resize(img,(im_width,im_height))\n",
        "        data.append(new_img)\n",
        "        progress = progress+1\n",
        "        labels.append(int(path[-2])-1)\n",
        "        if progress%100==0:\n",
        "            print('Progress '+str(progress)+' images :'  + path[-2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PY4OUlbe7fOf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Converting data and labels to numpy arrays and categorizing labels into 7 classes"
      ]
    },
    {
      "metadata": {
        "id": "J7f7wovKW92n",
        "colab_type": "code",
        "outputId": "0b5f9bae-a29a-4fc5-b1ca-78d94b930828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "data=np.array(data)\n",
        "print(data.shape)\n",
        "# data=data.reshape((data.shape)[0],(data.shape)[1],(data.shape)[2],1)\n",
        "print(data.shape)\n",
        "labels=np.array(labels)\n",
        "print(labels.shape)\n",
        "print(len(labels))\n",
        "print(labels)\n",
        "# labels.astype('uint8')\n",
        "labels = keras.utils.to_categorical(labels, 7)      "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13127, 150, 150, 3)\n",
            "(13127, 150, 150, 3)\n",
            "(13127,)\n",
            "13127\n",
            "[0 0 0 ... 6 6 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m4zdnvu371AM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Normalizing (Gave a jump in performance by atleast 2%)"
      ]
    },
    {
      "metadata": {
        "id": "FEFhRbGPW99E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = np.array(data).astype('uint8')\n",
        "data = data / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBwtpA2873AO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Shuffling Data and Labels on the same indices"
      ]
    },
    {
      "metadata": {
        "id": "YJXVrjjWW98a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def shuffle(a, b):\n",
        "    rng_state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hjNXhSJHW912",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    shuffle(data,labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0iu6r3qJ79MH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#The model begins here with 6 conv layers. \n",
        "###The first conv layer with 128 filters to extract the maximum low-level features, in this case the tumors. \n",
        "###The latter layers have decreasing filters emphasizing the impact of low-level features.\n",
        "###Used dropout to tackle overfitting.\n",
        "###Used Batch Normalization"
      ]
    },
    {
      "metadata": {
        "id": "-3gnNONt52hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab unexpectedly stopped after 182nd iteration, and would not have resumed from the epoch\n",
        "\n",
        "##spliting training and validation data in the fit function."
      ]
    },
    {
      "metadata": {
        "id": "C3UpSLgHSTRp",
        "colab_type": "code",
        "outputId": "e2bed256-048a-4044-c92f-a6b6e09c7414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7128
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=128,input_shape=(150, 150, 3),activation=\"relu\",padding=\"valid\"))\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=64,activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=32,activation=\"relu\",padding=\"same\"))\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=32,activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=16,activation=\"relu\",padding=\"same\"))\n",
        "model.add(Conv2D(kernel_size=(3,3),filters=16,activation=\"relu\",padding=\"same\"))\n",
        "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.75))\n",
        "model.add(Dense(100,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "history = model.fit(data,labels,\n",
        "          batch_size=100,\n",
        "          epochs=200,\n",
        "          verbose=1,\n",
        "          shuffle=True,\n",
        "          validation_split=0.3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 148, 148, 128)     3584      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 148, 148, 64)      73792     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 74, 74, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 74, 74, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 37, 37, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 37, 37, 16)        4624      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 37, 37, 16)        2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 18, 18, 16)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5184)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5184)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               518500    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 707       \n",
            "=================================================================\n",
            "Total params: 631,239\n",
            "Trainable params: 631,239\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 9188 samples, validate on 3939 samples\n",
            "Epoch 1/200\n",
            "9188/9188 [==============================] - 96s 10ms/step - loss: 1.8556 - acc: 0.2283 - val_loss: 1.7234 - val_acc: 0.3242\n",
            "Epoch 2/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.6752 - acc: 0.3512 - val_loss: 1.5458 - val_acc: 0.4384\n",
            "Epoch 3/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.5313 - acc: 0.4362 - val_loss: 1.3594 - val_acc: 0.5024\n",
            "Epoch 4/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.3983 - acc: 0.4947 - val_loss: 1.2626 - val_acc: 0.5390\n",
            "Epoch 5/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.2960 - acc: 0.5276 - val_loss: 1.1393 - val_acc: 0.5834\n",
            "Epoch 6/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.2149 - acc: 0.5646 - val_loss: 1.1199 - val_acc: 0.5897\n",
            "Epoch 7/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.1760 - acc: 0.5768 - val_loss: 1.0867 - val_acc: 0.6103\n",
            "Epoch 8/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.1406 - acc: 0.5871 - val_loss: 1.0487 - val_acc: 0.6128\n",
            "Epoch 9/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.1009 - acc: 0.6037 - val_loss: 1.0629 - val_acc: 0.6179\n",
            "Epoch 10/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.0479 - acc: 0.6276 - val_loss: 0.9425 - val_acc: 0.6522\n",
            "Epoch 11/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 1.0155 - acc: 0.6399 - val_loss: 0.9163 - val_acc: 0.6636\n",
            "Epoch 12/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.9856 - acc: 0.6515 - val_loss: 0.9179 - val_acc: 0.6669\n",
            "Epoch 13/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.9418 - acc: 0.6652 - val_loss: 0.9424 - val_acc: 0.6723\n",
            "Epoch 14/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.9192 - acc: 0.6679 - val_loss: 0.8446 - val_acc: 0.6921\n",
            "Epoch 15/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.8740 - acc: 0.6835 - val_loss: 0.7992 - val_acc: 0.7174\n",
            "Epoch 16/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.8559 - acc: 0.6938 - val_loss: 0.7493 - val_acc: 0.7431\n",
            "Epoch 17/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.8198 - acc: 0.7048 - val_loss: 0.7431 - val_acc: 0.7433\n",
            "Epoch 18/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.8139 - acc: 0.7092 - val_loss: 0.7234 - val_acc: 0.7520\n",
            "Epoch 19/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.7730 - acc: 0.7266 - val_loss: 0.6832 - val_acc: 0.7532\n",
            "Epoch 20/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.7505 - acc: 0.7317 - val_loss: 0.6578 - val_acc: 0.7629\n",
            "Epoch 21/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.7322 - acc: 0.7344 - val_loss: 0.6615 - val_acc: 0.7603\n",
            "Epoch 22/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6967 - acc: 0.7485 - val_loss: 0.6035 - val_acc: 0.7878\n",
            "Epoch 23/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6949 - acc: 0.7522 - val_loss: 0.6933 - val_acc: 0.7444\n",
            "Epoch 24/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6733 - acc: 0.7570 - val_loss: 0.6274 - val_acc: 0.7814\n",
            "Epoch 25/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6339 - acc: 0.7774 - val_loss: 0.5613 - val_acc: 0.8020\n",
            "Epoch 26/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6336 - acc: 0.7697 - val_loss: 0.6567 - val_acc: 0.7685\n",
            "Epoch 27/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6057 - acc: 0.7819 - val_loss: 0.5815 - val_acc: 0.8035\n",
            "Epoch 28/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.6065 - acc: 0.7776 - val_loss: 0.5649 - val_acc: 0.7984\n",
            "Epoch 29/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.5881 - acc: 0.7890 - val_loss: 0.5260 - val_acc: 0.8152\n",
            "Epoch 30/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.5733 - acc: 0.7963 - val_loss: 0.5431 - val_acc: 0.8154\n",
            "Epoch 31/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.5708 - acc: 0.7991 - val_loss: 0.4995 - val_acc: 0.8365\n",
            "Epoch 32/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.5380 - acc: 0.8071 - val_loss: 0.4997 - val_acc: 0.8256\n",
            "Epoch 33/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.5338 - acc: 0.8079 - val_loss: 0.5160 - val_acc: 0.8213\n",
            "Epoch 34/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.5267 - acc: 0.8141 - val_loss: 0.4916 - val_acc: 0.8390\n",
            "Epoch 35/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4888 - acc: 0.8262 - val_loss: 0.4680 - val_acc: 0.8454\n",
            "Epoch 36/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.4820 - acc: 0.8272 - val_loss: 0.4388 - val_acc: 0.8510\n",
            "Epoch 37/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.4829 - acc: 0.8298 - val_loss: 0.4543 - val_acc: 0.8479\n",
            "Epoch 38/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4960 - acc: 0.8247 - val_loss: 0.5092 - val_acc: 0.8350\n",
            "Epoch 39/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4785 - acc: 0.8269 - val_loss: 0.4601 - val_acc: 0.8403\n",
            "Epoch 40/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4662 - acc: 0.8315 - val_loss: 0.4437 - val_acc: 0.8510\n",
            "Epoch 41/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4677 - acc: 0.8353 - val_loss: 0.4358 - val_acc: 0.8571\n",
            "Epoch 42/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4342 - acc: 0.8440 - val_loss: 0.4240 - val_acc: 0.8543\n",
            "Epoch 43/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.4239 - acc: 0.8426 - val_loss: 0.4711 - val_acc: 0.8390\n",
            "Epoch 44/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4501 - acc: 0.8406 - val_loss: 0.4033 - val_acc: 0.8660\n",
            "Epoch 45/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.4263 - acc: 0.8486 - val_loss: 0.4107 - val_acc: 0.8642\n",
            "Epoch 46/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4188 - acc: 0.8540 - val_loss: 0.3957 - val_acc: 0.8766\n",
            "Epoch 47/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.4020 - acc: 0.8576 - val_loss: 0.4117 - val_acc: 0.8677\n",
            "Epoch 48/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3941 - acc: 0.8626 - val_loss: 0.4135 - val_acc: 0.8614\n",
            "Epoch 49/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3876 - acc: 0.8620 - val_loss: 0.4044 - val_acc: 0.8718\n",
            "Epoch 50/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3820 - acc: 0.8645 - val_loss: 0.3978 - val_acc: 0.8670\n",
            "Epoch 51/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3812 - acc: 0.8659 - val_loss: 0.3631 - val_acc: 0.8769\n",
            "Epoch 52/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3862 - acc: 0.8685 - val_loss: 0.3996 - val_acc: 0.8693\n",
            "Epoch 53/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3721 - acc: 0.8717 - val_loss: 0.3532 - val_acc: 0.8786\n",
            "Epoch 54/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3638 - acc: 0.8730 - val_loss: 0.3974 - val_acc: 0.8753\n",
            "Epoch 55/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3491 - acc: 0.8809 - val_loss: 0.3447 - val_acc: 0.8911\n",
            "Epoch 56/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3371 - acc: 0.8828 - val_loss: 0.3670 - val_acc: 0.8814\n",
            "Epoch 57/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3469 - acc: 0.8808 - val_loss: 0.3737 - val_acc: 0.8817\n",
            "Epoch 58/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3485 - acc: 0.8771 - val_loss: 0.3428 - val_acc: 0.8888\n",
            "Epoch 59/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3514 - acc: 0.8777 - val_loss: 0.3607 - val_acc: 0.8886\n",
            "Epoch 60/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.3207 - acc: 0.8903 - val_loss: 0.3579 - val_acc: 0.8858\n",
            "Epoch 61/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3224 - acc: 0.8837 - val_loss: 0.3339 - val_acc: 0.8944\n",
            "Epoch 62/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3397 - acc: 0.8811 - val_loss: 0.3533 - val_acc: 0.8863\n",
            "Epoch 63/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3287 - acc: 0.8875 - val_loss: 0.3194 - val_acc: 0.8974\n",
            "Epoch 64/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2978 - acc: 0.8973 - val_loss: 0.3574 - val_acc: 0.8865\n",
            "Epoch 65/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3168 - acc: 0.8913 - val_loss: 0.3234 - val_acc: 0.9005\n",
            "Epoch 66/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3156 - acc: 0.8905 - val_loss: 0.3525 - val_acc: 0.8853\n",
            "Epoch 67/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3303 - acc: 0.8811 - val_loss: 0.3491 - val_acc: 0.8901\n",
            "Epoch 68/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3137 - acc: 0.8936 - val_loss: 0.4017 - val_acc: 0.8687\n",
            "Epoch 69/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.3219 - acc: 0.8911 - val_loss: 0.3139 - val_acc: 0.9073\n",
            "Epoch 70/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2778 - acc: 0.9052 - val_loss: 0.3192 - val_acc: 0.9000\n",
            "Epoch 71/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2855 - acc: 0.8967 - val_loss: 0.2703 - val_acc: 0.9180\n",
            "Epoch 72/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2751 - acc: 0.9017 - val_loss: 0.3027 - val_acc: 0.9025\n",
            "Epoch 73/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2693 - acc: 0.9068 - val_loss: 0.2818 - val_acc: 0.9157\n",
            "Epoch 74/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2677 - acc: 0.9071 - val_loss: 0.2953 - val_acc: 0.9084\n",
            "Epoch 75/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2653 - acc: 0.9101 - val_loss: 0.2778 - val_acc: 0.9162\n",
            "Epoch 76/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2557 - acc: 0.9109 - val_loss: 0.2820 - val_acc: 0.9109\n",
            "Epoch 77/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2578 - acc: 0.9114 - val_loss: 0.3074 - val_acc: 0.9076\n",
            "Epoch 78/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2586 - acc: 0.9105 - val_loss: 0.2875 - val_acc: 0.9104\n",
            "Epoch 79/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2515 - acc: 0.9116 - val_loss: 0.3053 - val_acc: 0.9015\n",
            "Epoch 80/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2538 - acc: 0.9112 - val_loss: 0.3140 - val_acc: 0.9051\n",
            "Epoch 81/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2589 - acc: 0.9109 - val_loss: 0.2880 - val_acc: 0.9139\n",
            "Epoch 82/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2644 - acc: 0.9081 - val_loss: 0.3162 - val_acc: 0.9066\n",
            "Epoch 83/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2324 - acc: 0.9192 - val_loss: 0.2709 - val_acc: 0.9157\n",
            "Epoch 84/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2322 - acc: 0.9189 - val_loss: 0.2591 - val_acc: 0.9203\n",
            "Epoch 85/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2316 - acc: 0.9252 - val_loss: 0.2534 - val_acc: 0.9251\n",
            "Epoch 86/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2276 - acc: 0.9209 - val_loss: 0.2451 - val_acc: 0.9238\n",
            "Epoch 87/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2352 - acc: 0.9208 - val_loss: 0.2458 - val_acc: 0.9264\n",
            "Epoch 88/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2301 - acc: 0.9219 - val_loss: 0.2574 - val_acc: 0.9221\n",
            "Epoch 89/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2183 - acc: 0.9252 - val_loss: 0.2645 - val_acc: 0.9243\n",
            "Epoch 90/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2257 - acc: 0.9227 - val_loss: 0.2600 - val_acc: 0.9218\n",
            "Epoch 91/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2239 - acc: 0.9215 - val_loss: 0.2722 - val_acc: 0.9160\n",
            "Epoch 92/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2248 - acc: 0.9205 - val_loss: 0.2563 - val_acc: 0.9246\n",
            "Epoch 93/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2207 - acc: 0.9256 - val_loss: 0.3033 - val_acc: 0.9023\n",
            "Epoch 94/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2283 - acc: 0.9215 - val_loss: 0.2499 - val_acc: 0.9266\n",
            "Epoch 95/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2250 - acc: 0.9228 - val_loss: 0.2625 - val_acc: 0.9228\n",
            "Epoch 96/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2341 - acc: 0.9215 - val_loss: 0.2760 - val_acc: 0.9150\n",
            "Epoch 97/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2295 - acc: 0.9208 - val_loss: 0.3082 - val_acc: 0.9073\n",
            "Epoch 98/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2511 - acc: 0.9145 - val_loss: 0.2829 - val_acc: 0.9127\n",
            "Epoch 99/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2154 - acc: 0.9250 - val_loss: 0.2425 - val_acc: 0.9317\n",
            "Epoch 100/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2101 - acc: 0.9283 - val_loss: 0.2533 - val_acc: 0.9231\n",
            "Epoch 101/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2014 - acc: 0.9335 - val_loss: 0.3293 - val_acc: 0.9000\n",
            "Epoch 102/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2522 - acc: 0.9155 - val_loss: 0.2305 - val_acc: 0.9332\n",
            "Epoch 103/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1827 - acc: 0.9388 - val_loss: 0.2480 - val_acc: 0.9271\n",
            "Epoch 104/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1880 - acc: 0.9340 - val_loss: 0.2706 - val_acc: 0.9226\n",
            "Epoch 105/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1987 - acc: 0.9321 - val_loss: 0.2577 - val_acc: 0.9216\n",
            "Epoch 106/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2283 - acc: 0.9259 - val_loss: 0.2492 - val_acc: 0.9264\n",
            "Epoch 107/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1912 - acc: 0.9369 - val_loss: 0.2568 - val_acc: 0.9274\n",
            "Epoch 108/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1884 - acc: 0.9354 - val_loss: 0.3023 - val_acc: 0.9106\n",
            "Epoch 109/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1816 - acc: 0.9393 - val_loss: 0.2554 - val_acc: 0.9249\n",
            "Epoch 110/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.1973 - acc: 0.9344 - val_loss: 0.2509 - val_acc: 0.9264\n",
            "Epoch 111/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1797 - acc: 0.9394 - val_loss: 0.3479 - val_acc: 0.8982\n",
            "Epoch 112/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2097 - acc: 0.9283 - val_loss: 0.2208 - val_acc: 0.9340\n",
            "Epoch 113/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1788 - acc: 0.9412 - val_loss: 0.2411 - val_acc: 0.9307\n",
            "Epoch 114/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1860 - acc: 0.9370 - val_loss: 0.2591 - val_acc: 0.9266\n",
            "Epoch 115/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2230 - acc: 0.9270 - val_loss: 0.2196 - val_acc: 0.9375\n",
            "Epoch 116/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1973 - acc: 0.9313 - val_loss: 0.2356 - val_acc: 0.9340\n",
            "Epoch 117/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1745 - acc: 0.9384 - val_loss: 0.2549 - val_acc: 0.9353\n",
            "Epoch 118/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1763 - acc: 0.9400 - val_loss: 0.2215 - val_acc: 0.9345\n",
            "Epoch 119/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1809 - acc: 0.9416 - val_loss: 0.2727 - val_acc: 0.9210\n",
            "Epoch 120/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1932 - acc: 0.9333 - val_loss: 0.2319 - val_acc: 0.9337\n",
            "Epoch 121/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1649 - acc: 0.9462 - val_loss: 0.3081 - val_acc: 0.9155\n",
            "Epoch 122/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1936 - acc: 0.9361 - val_loss: 0.2196 - val_acc: 0.9370\n",
            "Epoch 123/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1597 - acc: 0.9433 - val_loss: 0.2291 - val_acc: 0.9320\n",
            "Epoch 124/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1699 - acc: 0.9456 - val_loss: 0.2246 - val_acc: 0.9348\n",
            "Epoch 125/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1553 - acc: 0.9485 - val_loss: 0.2359 - val_acc: 0.9375\n",
            "Epoch 126/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1518 - acc: 0.9460 - val_loss: 0.2273 - val_acc: 0.9350\n",
            "Epoch 127/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2095 - acc: 0.9300 - val_loss: 0.2286 - val_acc: 0.9350\n",
            "Epoch 128/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1746 - acc: 0.9401 - val_loss: 0.2311 - val_acc: 0.9345\n",
            "Epoch 129/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1791 - acc: 0.9387 - val_loss: 0.2706 - val_acc: 0.9249\n",
            "Epoch 130/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.2106 - acc: 0.9295 - val_loss: 0.2513 - val_acc: 0.9266\n",
            "Epoch 131/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1733 - acc: 0.9410 - val_loss: 0.2210 - val_acc: 0.9325\n",
            "Epoch 132/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1588 - acc: 0.9437 - val_loss: 0.2096 - val_acc: 0.9406\n",
            "Epoch 133/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1503 - acc: 0.9506 - val_loss: 0.2230 - val_acc: 0.9406\n",
            "Epoch 134/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1517 - acc: 0.9478 - val_loss: 0.2772 - val_acc: 0.9231\n",
            "Epoch 135/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1747 - acc: 0.9398 - val_loss: 0.2113 - val_acc: 0.9431\n",
            "Epoch 136/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1726 - acc: 0.9402 - val_loss: 0.2162 - val_acc: 0.9444\n",
            "Epoch 137/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1521 - acc: 0.9485 - val_loss: 0.2285 - val_acc: 0.9322\n",
            "Epoch 138/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.1423 - acc: 0.9507 - val_loss: 0.2482 - val_acc: 0.9345\n",
            "Epoch 139/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1785 - acc: 0.9413 - val_loss: 0.2112 - val_acc: 0.9414\n",
            "Epoch 140/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1651 - acc: 0.9468 - val_loss: 0.2258 - val_acc: 0.9363\n",
            "Epoch 141/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1445 - acc: 0.9499 - val_loss: 0.2136 - val_acc: 0.9414\n",
            "Epoch 142/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1506 - acc: 0.9497 - val_loss: 0.2029 - val_acc: 0.9457\n",
            "Epoch 143/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1392 - acc: 0.9540 - val_loss: 0.2893 - val_acc: 0.9241\n",
            "Epoch 144/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1898 - acc: 0.9379 - val_loss: 0.1984 - val_acc: 0.9393\n",
            "Epoch 145/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1431 - acc: 0.9524 - val_loss: 0.2004 - val_acc: 0.9421\n",
            "Epoch 146/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1688 - acc: 0.9417 - val_loss: 0.2331 - val_acc: 0.9307\n",
            "Epoch 147/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1523 - acc: 0.9473 - val_loss: 0.2058 - val_acc: 0.9353\n",
            "Epoch 148/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1644 - acc: 0.9448 - val_loss: 0.2362 - val_acc: 0.9299\n",
            "Epoch 149/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1601 - acc: 0.9481 - val_loss: 0.2835 - val_acc: 0.9155\n",
            "Epoch 150/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1521 - acc: 0.9495 - val_loss: 0.1719 - val_acc: 0.9454\n",
            "Epoch 151/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1290 - acc: 0.9576 - val_loss: 0.2059 - val_acc: 0.9408\n",
            "Epoch 152/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1373 - acc: 0.9559 - val_loss: 0.2205 - val_acc: 0.9373\n",
            "Epoch 153/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1478 - acc: 0.9502 - val_loss: 0.1972 - val_acc: 0.9436\n",
            "Epoch 154/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1436 - acc: 0.9539 - val_loss: 0.2179 - val_acc: 0.9419\n",
            "Epoch 155/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1466 - acc: 0.9500 - val_loss: 0.2035 - val_acc: 0.9424\n",
            "Epoch 156/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1596 - acc: 0.9485 - val_loss: 0.2710 - val_acc: 0.9279\n",
            "Epoch 157/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1385 - acc: 0.9554 - val_loss: 0.2260 - val_acc: 0.9360\n",
            "Epoch 158/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1319 - acc: 0.9561 - val_loss: 0.2399 - val_acc: 0.9340\n",
            "Epoch 159/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1315 - acc: 0.9577 - val_loss: 0.2358 - val_acc: 0.9360\n",
            "Epoch 160/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1550 - acc: 0.9473 - val_loss: 0.2116 - val_acc: 0.9337\n",
            "Epoch 161/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1446 - acc: 0.9504 - val_loss: 0.2435 - val_acc: 0.9350\n",
            "Epoch 162/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1305 - acc: 0.9556 - val_loss: 0.1972 - val_acc: 0.9414\n",
            "Epoch 163/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1395 - acc: 0.9543 - val_loss: 0.2086 - val_acc: 0.9431\n",
            "Epoch 164/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1205 - acc: 0.9584 - val_loss: 0.2016 - val_acc: 0.9408\n",
            "Epoch 165/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1450 - acc: 0.9536 - val_loss: 0.2086 - val_acc: 0.9441\n",
            "Epoch 166/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1647 - acc: 0.9456 - val_loss: 0.2313 - val_acc: 0.9360\n",
            "Epoch 167/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1443 - acc: 0.9518 - val_loss: 0.2270 - val_acc: 0.9386\n",
            "Epoch 168/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1609 - acc: 0.9461 - val_loss: 0.1931 - val_acc: 0.9502\n",
            "Epoch 169/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1116 - acc: 0.9625 - val_loss: 0.2218 - val_acc: 0.9416\n",
            "Epoch 170/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1295 - acc: 0.9536 - val_loss: 0.2316 - val_acc: 0.9368\n",
            "Epoch 171/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1461 - acc: 0.9508 - val_loss: 0.2057 - val_acc: 0.9452\n",
            "Epoch 172/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1633 - acc: 0.9458 - val_loss: 0.2164 - val_acc: 0.9429\n",
            "Epoch 173/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1277 - acc: 0.9574 - val_loss: 0.1746 - val_acc: 0.9472\n",
            "Epoch 174/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1235 - acc: 0.9602 - val_loss: 0.1865 - val_acc: 0.9507\n",
            "Epoch 175/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1331 - acc: 0.9569 - val_loss: 0.1950 - val_acc: 0.9513\n",
            "Epoch 176/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1277 - acc: 0.9580 - val_loss: 0.1925 - val_acc: 0.9497\n",
            "Epoch 177/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1434 - acc: 0.9524 - val_loss: 0.1728 - val_acc: 0.9528\n",
            "Epoch 178/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1202 - acc: 0.9597 - val_loss: 0.2443 - val_acc: 0.9401\n",
            "Epoch 179/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1158 - acc: 0.9626 - val_loss: 0.1953 - val_acc: 0.9520\n",
            "Epoch 180/200\n",
            "9188/9188 [==============================] - 81s 9ms/step - loss: 0.1405 - acc: 0.9533 - val_loss: 0.2242 - val_acc: 0.9426\n",
            "Epoch 181/200\n",
            "9188/9188 [==============================] - 82s 9ms/step - loss: 0.1378 - acc: 0.9532 - val_loss: 0.2144 - val_acc: 0.9429\n",
            "Epoch 182/200\n",
            "8400/9188 [==========================>...] - ETA: 6s - loss: 0.1489 - acc: 0.9525"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4gQvUNfe6K1x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#The pattern would have been the same on the 200th epoch. \n",
        "##Final output might have been:    Train: 96%, Test: 94%"
      ]
    },
    {
      "metadata": {
        "id": "8IbBxkZN9G0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Wasn't able to plot the graph due the disconnection of colab's kernel"
      ]
    },
    {
      "metadata": {
        "id": "4oLrz0KrXpcj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "#loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}